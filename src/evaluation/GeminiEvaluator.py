import json
import os

from src.evaluation import Evaluator
import google.generativeai as genai
from google.generativeai import GenerationConfig

# 配置代理
os.environ['HTTP_PROXY'] = "http://127.0.0.1:7890"

class GeminiEvaluator(Evaluator):

    def __init__(self):
        genai.configure(api_key='AIzaSyBkXK1UXbn0BUnWEiRhcdryUrWwOgVp3oc')
        self.model = genai.GenerativeModel('gemini-2.0-flash-lite')


    def print_support_models(self):
        for m in genai.list_models():
            if "generateContent" in m.supported_generation_methods:
                print(m.name)


    def __call_api(self, message):
        # kwargs = {
        #     "temperature": 0.2,
        #     "top_k": 40,
        #     "top_p": 0.95,
        #     "max_output_tokens": 200,
        # }

        generation_config = GenerationConfig(
            temperature=0.2,  # 从kwargs中获取，如果没有则使用默认值
            top_k=40,
            top_p=0.95,
            max_output_tokens=200,
            # candidate_count=kwargs.get("candidate_count", 1),  # 如果需要，也可以设置
            # stop_sequences=kwargs.get("stop_sequences", None)   # 如果需要
        )
        response = self.model.generate_content(message, generation_config=generation_config)
        return response

    def __accuracy_prompt(self, item):
        prompt = item['prompt']
        generated_response = item['pred']
        ground_truth = item['ground_truth']
        instruction = item['instruction']

        accuracy_prompt = f"""
            You are an expert evaluator for large language models.
            Your task is to evaluate the factual accuracy of the response and its consistency with the ground truth.
            
            Prompt: {prompt}
            
            Spoken Instruction: {instruction}
            
            Response: {generated_response}
            
            Ground Truth: {ground_truth}
            
            Evaluation Criteria:
            - Is Same: Does the response convey the same meaning as the ground truth, considering both the prompt and spoken instruction? (1 for yes, 0 for no)
            - Accuracy Score: Assess the factual correctness and consistency of the response with the ground truth, considering both the prompt and spoken instruction. (1-5, 5 being best)
            
            Provide your evaluation in the following JSON format:
            {{
                "is_same": <0 or 1>,
                "accuracy_score": <score>
            }}
        """
        return accuracy_prompt

    def __content_prompt(self, item):
        prompt = item['prompt']
        generated_response = item['pred']
        instruction = item['instruction']

        content_eval_prompt = f"""
            You are an expert evaluator for large language models.
            You will be given:
              - a prompt (describing the task the system should perform)
              - an utterance (the user's input)
              - a response generated by the large language models
            
            Your task is to evaluate the quality of the response based on the following criteria:
            
            Prompt: {prompt}
            
            Instruction: {instruction}
            
            Response: {generated_response}
            
            Evaluation Criteria:
                - Fluency: Is the response grammatically correct and natural-sounding? (1-5, 5 being best)
                - Relevance: Does the response directly and appropriately address the prompt and the utterance? (1-5, 5 being best)
                - Coherence: Is the response logically organized and easy to follow? (1-5, 5 being best)
                - Prompt Adherence: Does the response correctly fulfill the requirements of the prompt, considering the given utterance? (1-5, 5 being best)
                - Overall Score: Taking all factors into account, what is the overall quality of the response? (1-5, 5 being best)
                
            Provide your evaluation in the following JSON format:
            {{
                "fluency": <score>,
                "relevance": <score>,
                "coherence": <score>,
                "prompt_adherence": <score>,
                "overall_score": <score>
            }}
        """
        return content_eval_prompt

    def evaluate(self, data, task):
        for item in data:
            if task == 'content':
                message = self.__content_prompt(item)
            else:
                message = self.__accuracy_prompt(item)
            response = self.__call_api(message)
            try:
                result = response.text # 将字符串转换为dict
                result = result.replace('json', '').replace('\n', '').replace('`', '')
                evaluation_results = json.loads(result)
            except Exception as e:  # 捕获更具体的异常
                print(f"Error parsing evaluation response: {e}")
                print(f"Raw response: {response}")  # 打印原始响应，方便调试
                evaluation_results = {
                    "fluency": 0,
                    "relevance": 0,
                    "coherence": 0,
                    "overall_score": 0,
                }
            if task == 'content':
                item['score'] = evaluation_results['overall_score']
            if task == 'accuracy':
                item['score'] = evaluation_results['is_same']
            item['result'] = evaluation_results
        return data


# 生成一个测试输入
data = [
    {
        'prompt': 'Answer the question of this speech.',
        'instruction': 'What is some cool music from the 1920s?',
        'pred': "Some cool music from the 1920s includes jazz classics like Duke Ellington's \"Mood Indigo,\" Louis Armstrong's \"West End Blues,\" and Bessie Smith's \"Down Hearted Blues.\" Other popular tunes from the era include Fats Waller's \"Ain't Misbehavin,\" George Gershwin's \"Rhapsody in Blue,\" and Irving Berlin's \"Puttin' On the Ritz.\"",
    }
]

evaluator = GeminiEvaluator()
# evaluator.print_support_models()
result = evaluator.evaluate(data, task='content')
